'''
이진 분류, 로지스틱 회귀에서 널리 사용하는 손실함수로 아래와 같은 식을 사용한다.
BCE(p,y)=−[ylog(p)+(1−y)log(1−p)]

p는 모델의 예측 확률이고
y는 정답 라벨이다.

================배경 지식: likelihood ==========================

ML에서 우도 : 내 모델이 정답 y를 맞출 확률로 학습하며 점점 만들어져간다.
            이 값이 클수록 모델이 좋음 => 그래서 우도를 최대화(MLE : max likelyhood Estimate)하는 방향으로 학습함

=================== 데이터 하나짜리에서 Likelihood ===================

정의:
  - 확률 p일 때, 실제 값 y(0 또는 1)가 나올 확률.

수식:
  P(y | p) = p^y * (1 - p)^(1 - y)

해석:
  - y = 1이면 likelihood = p
  - y = 0이면 likelihood = 1 - p


=================== 여러 데이터에서의 Likelihood ===================

여러 데이터(샘플 n개)가 독립이라고 가정할 때,
전체 likelihood는 각 샘플 likelihood의 곱(product).

수식:
  L(w) = Π ( p_i^y_i * (1 - p_i)^(1 - y_i) )         # 참고로 L(w) = Likelihood(들어오는 가중치 w)이다^^ 단개 데이터때와 다르게 여러 데이터 있는 경우엔 w가 맞는지 확인하는 함수라고 볼 수 있음.
        (i = 1 to n) 

각 샘플 i에 대해, 모델이 계산한 확률을 p_i 라 하고, 실제 정답을 y_i (0 또는 1) 라고 할 때,

개별 샘플의 likelihood는:
  p_i (정답이 1일 때)
  1 - p_i (정답이 0일 때)

이 둘을 하나의 식으로 묶으면:
  p_i^(y_i) * (1 - p_i)^(1 - y_i)
( * 를 기준으로 앞이나 뒤가 y_i가 1이냐 0이냐에 따라 값이 앞이나 뒤만 살아남겠죠?)

그래서 전체 데이터의 likelihood는 다음처럼 생긴다:
여기서 p_i는 어떻게 나오느냐?

  p_i = sigmoid( w^T * x_i + b )
  

즉, 각 샘플 x_i를 w(가중치)에 곱해서 나온 값에 시그모이드를 통과시키면
p_i라는 "모델이 예측한 확률"이 되고, 이 확률을 가지고 likelihood를 계산한다.


=================== 로그 Likelihood ===================

곱은 값이 너무 작아져서 계산이 불안정해서 log를 취함.

log L(w)
  = Σ [ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ]


=================== 음의 로그 Likelihood (Negative Log-Likelihood) ===================

최적화는 "최소화" 형태가 일반적 → log likelihood에 마이너스(-)를 붙여 loss로 정의.

NLL(w)
  = - Σ [ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ]

이것이 바로 우리가 알고 있는 Binary Cross Entropy(BCE) Loss.

미니배치 평균 형태:
  BCE = -(1/n) * Σ [ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ]



======================= BCE 수식 전개 =============================

BCE(p,y)=−[ylog(p)+(1−y)log(1−p)]

y = 0 일때 -(1−y)log(1−p) = -log(1−p)가 살아남고, y = 1이면 -ylog(p)=-log(p) 이 항이 살아남겠죠?

'''