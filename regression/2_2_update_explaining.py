'''
모델 훈련시 각 가중치 w,b는 왜 아래와 같은 식으로 업데이트가 될까?
        w -= lr * dw
        b -= lr * db

        

---------------------------------------------------------------------------------
dw = dL/dw
=> w 값을 아주 조금 증가(+)시켰을 때 L가 얼마나 증가(또는 감소)하는지를 나타내는 값.
   즉, L이 w 축 방향으로 변화하는 기울기(증가 방향의 성분).

db = dL/db
=> b 값을 아주 조금 증가(+)시켰을 때 L가 얼마나 증가(또는 감소)하는지를 나타내는 값.
   즉, L이 b 축 방향으로 변화하는 기울기(증가 방향의 성분).

dw와 db는 각각 L(w, b)라는 2차원 Loss 함수가
'w 방향', 'b 방향'으로 어떻게 증가하려 하는지를 나타내는 "증가 방향의 성분(gradient component)"이다.

Gradient Descent는 이 증가 방향의 '반대 방향'으로 이동하여
Loss L를 줄인다.

따라서 파라미터 업데이트는 다음과 같이 이루어진다:

    w = w - lr * dw
    b = b - lr * db
---------------------------------------------------------------------------------
        
우리는 MSE을 사용하므로(**2하므로) 무조건 대칭적 볼록 함수일수밖에 없는데,
이럴 경우 우리가 구한 dw와 db를 왼쪽 혹은 우측으로 움직여 L이 0이 되는 것이 목표이므로 기존 w,b에서
lr만큼 천천히 빼주며 업데이트 하는 것이다

'''

