'''
linear regression에 본격적으로 들어가기 전에, 업데이트 값인 dw, db에 대해 알아보기.

dw = 가중치 w를 얼마나, 어느 방향으로 움직여야 loss가 줄어드는지 알려주는 값 : 기울기 에러 정도보고 수정
db = 편향 b를 얼마나 움직여야 loss가 줄어드는지 알려주는 값 : 전체 위치 에러 정도보고 수정
'''
import numpy as np

#  예제 데이터 (x=2, y=10)
X = np.array([[2.0]]) # 인풋
y = np.array([10.0])  # 정답

#  초기 w, b 
w = 1.0
b = 0.0

lr = 0.1  # learning rate

print("===== 초기 상태 =====")
print(f"w = {w}, b = {b}")
print()

# ---------------------------
# 1. forward (예측)
# ---------------------------
y_pred = X @ np.array([w]) + b     # linear combination으로 선형 그래프 그린다.
print("1) 예측값 (y_pred):", y_pred)

# ---------------------------
# 2. error (예측 - 실제)
# ---------------------------
error = y_pred - y
print("2) error (y_pred - y):", error)

# ---------------------------
# 3. loss (참고용)
# ---------------------------
loss = np.mean(error**2)
print("3) loss (MSE):", loss)

# ---------------------------
# 4. gradient 계산!!!
# ---------------------------

# dw = (2/N) * X.T @ error
dw = 2 * (X.T @ error) / len(X)
db = 2 * np.sum(error) / len(X)

print("4) dw:", dw)
print("   db:", db)

# ---------------------------
# 5. update 단계
# ---------------------------
w_new = w - lr * dw
b_new = b - lr * db

print()
print("===== 업데이트 후 =====")
print(f"w_new = {w_new}")
print(f"b_new = {b_new}")



###########################################################################
# 확장하기
###########################################################################
'''
여러 데이터에서 dw, db가 어떻게 계산되는지 확인하기
'''

# 예제 데이터 3개
X = np.array([[2.0],
              [5.0],
              [7.0]])   # shape (3,1)
y = np.array([10.0, 22.0, 30.0])  # shape (3,)

# 초기 w, b
w = 1.0
b = 0.0
lr = 0.1

print("===== 초기 상태 =====")
print(f"w = {w}, b = {b}")
print()

# ---------------------------
# 1. forward (예측)
# ---------------------------
y_pred = X @ np.array([w]) + b
print("1) 예측값 (y_pred):", y_pred)

# ---------------------------
# 2. error (예측 - 실제)
# ---------------------------
error = y_pred - y
print("2) error:", error)

# ---------------------------
# 3. loss (참고용)
# ---------------------------
loss = np.mean(error**2)
print("3) loss (MSE):", loss)

# ---------------------------
# 4. gradient 계산
# ---------------------------

N = len(X)

dw = 2 * (X.T @ error) / N  
db = 2 * np.sum(error) / N

print("4) dw:", dw)
print("   db:", db)

# ---------------------------
# 5. update 단계
# ---------------------------
w_new = w - lr * dw
b_new = b - lr * db

print()
print("===== 업데이트 후 =====")
print(f"w_new = {w_new}")
print(f"b_new = {b_new}")



###########################################################################
# 보충설명 : dw 계산의미
###########################################################################

'''
미분법칙1.
        (u**2)' = 2u*u'
        (x**2)' = 2x

미분법칙2.
        (ax)' = a
        (5x)' = 5

미분법칙3.
        (상수)' = 0
        (5)' = 0

위에 법칙을 생각하고 전개해보자 L이 줄어드는 방향으로 w를 수정하는 것이 목표!
L = (1/N)*sum(e**2)
=> dL_dw = around(1/N)*sum(e_i**2)_aw
=> (1/N)2e_i*around(e_i)_aw => 여기서 e_i는 y_pred = wx_i +b - y_i임
=> (1/N)_2e_i*x_i = (2/N)x_ie_i 

= 2 * (X.T @ error) / N  



'''
###########################################################################
# 보충설명 : db 계산의미
###########################################################################
'''

b는 그래프를 통째로 위/아래로 평행이동하는 역할
db = 2 * np.sum(error) / N   => 업데이트될 정도

error = y_pred - y 에서 error가 양수면 예측이 너무 크게 나온 것이고 음수면 예측이 너무 적었다는 의미로
즉, error들의 합(sum(error))은 전체 예측이 전체적으로 위인가? 아래인가?를 나타낸다.

L = (1/N)*sum(wx_i +b -y_i)**2     # 여기서 wx_i +b는 y_pred
=> around L/ around b = around (sum(wx_i +b -y_i)**2) _around b
=> (1/N)2(wx_i +b -y_i)*1
=> 2/N(wx_i +b -y_i)   # 여기서 (wx_i +b -y_i) = e
=> 2 * np.sum(error) / N

'''